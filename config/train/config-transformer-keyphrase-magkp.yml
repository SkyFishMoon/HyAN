model_type: keyphrase

data: data/keyphrase/magkp/magkp_train.one2many
save_model: models/magkp/magkp.one2one.transformer
save_checkpoint_steps: 10000
keep_checkpoint: 40
seed: 3435

encoder_type: transformer
decoder_type: transformer
word_vec_size: 512
rnn_size: 512
layers: 4

position_encoding: true

optim: adam
learning_rate: 2
param_init: 0
warmup_steps: 8000
decay_method: noam
label_smoothing: 0.1
adam_beta2: 0.998

batch_type: tokens
normalization: tokens
max_generator_batches: 2
accum_count: 4

# batch_size is actually: num_example * max(#word in src/tgt)
batch_size: 4096
#batch_size: 8192
#batch_size: 24576
valid_batch_size: 256

train_steps: 400000
valid_steps: 10000
report_every: 100

dropout: 0.2

share_embeddings: 'true'
copy_attn: 'true'
param_init_glorot: 'true'

log_file: output/magkp.one2one.transformer.log
log_file_level: DEBUG
exp: magkp-one2one-transformer-Layer4-Dim512-Emb512--Dropout0.2
tensorboard: 'true'
tensorboard_log_dir: runs/magkp.one2one.transformer/

world_size: 2
gpu_ranks:
- 0
- 1
#- 2
master_port: 10001